[project]
name = "graphrag-eval"
version = "5.3.2a0"
description = "For assessing question answering systems' final answers and intermediate steps, against a given set of questions, reference answers and steps."
authors = [
    { name = "Philip Ganchev", email = "philip.ganchev@graphwise.ai" },
    { name = "Aleksis Datseris", email = "aleksis.datseris@graphwise.ai" },
    { name = "Neli Hateva", email = "neli.hateva@graphwise.ai" },
]
readme = "README.md"
license = "Apache-2.0"
requires-python = ">=3.12,<3.13"

[project.urls]
repository = "https://github.com/Ontotext-AD/graphrag-eval"

[tool.poetry.dependencies]
python-dateutil = "2.9.0.post0"
ragas = { version = "0.4.3", optional = true }
langchain-openai = { version = "1.1.7", optional = true }

[tool.poetry.extras]
llm = ["ragas", "langchain-openai", "pyyaml"]

[tool.poetry.group.llm.dependencies]
ragas = "0.4.3"
langchain-openai = "1.1.7"
pyyaml = "6.0.3"

[tool.poetry.group.llm]
optional = true

[tool.poetry.group.test.dependencies]
pytest = "<10,>=9"
pytest-asyncio = "1.3.0"
pytest-cov = "<8,>=7"
jsonlines = "4.0.0"
pyyaml = "6.0.3"

[tool.poetry.group.test]
optional = true

[project.scripts]
answer-correctness = "graphrag_eval.answer_correctness:main"

[build-system]
requires = ["poetry-core>=2.0.0"]
build-backend = "poetry.core.masonry.api"
